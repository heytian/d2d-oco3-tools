{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxVvdWyV/VD54INmeBKNk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heytian/d2d-oco3-tools/blob/main/nc4_plot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j_bnrZ15jjc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. NC4 Files (from network) to CSV**\n",
        "\n",
        "This is a tool to filter for high quality Snapshot Area Maps (SAM)s, and to aggregate a single CO2 value for each SAM location (taking the median CO2), from a network location containing nc4 file links without having to download nc4 files to your local storage. The lat/lon is the center point of all the soundings with the same target_name.\n",
        "\n",
        "**IMPORTANT: Clear all outputs and end runtime session before saving to Colab or Github to avoid exposing your Earthdata credentials!**"
      ],
      "metadata": {
        "id": "_3h4ap-Fjk3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to write .netrc to Colab for your Earthdata credentials. RESTART SESSION AND CLEAR OUTPUTS AFTER USE!\n",
        "\n",
        "import getpass, os\n",
        "\n",
        "# Prompt user for Earthdata credentials\n",
        "u = getpass.getpass(\"Earthdata Username: \")\n",
        "p = getpass.getpass(\"Earthdata Password: \")\n",
        "\n",
        "# Write .netrc\n",
        "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "with open(netrc_path, \"w\") as f:\n",
        "    f.write(f\"machine urs.earthdata.nasa.gov\\n\"\n",
        "            f\"  login {u}\\n\"\n",
        "            f\"  password {p}\\n\")\n",
        "\n",
        "os.chmod(netrc_path, 0o600)\n",
        "\n",
        "# Cookie file\n",
        "cookie_path = os.path.expanduser(\"~/.urs_cookies\")\n",
        "open(cookie_path, \"a\").close()\n",
        "os.chmod(cookie_path, 0o600)\n",
        "\n",
        "print(\"Credentials loaded securely.\")\n"
      ],
      "metadata": {
        "id": "NGfCL9vykENY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the year under PRODUCT to amend range of data you want\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from getpass import getpass\n",
        "from urllib.parse import urljoin\n",
        "import numpy.lib.recfunctions as rfn\n",
        "\n",
        "\n",
        "# CONFIG: GESDISC BASE URL\n",
        "BASE_URL = \"https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/\"\n",
        "# Example products: for CO2 levels \"OCO3_L2_Lite_FP.11r/\", for SIF \"OCO3_L2_Lite_SIF.11r/\"\n",
        "PRODUCT = \"OCO3_L2_Lite_SIF.11r/2025/\" #change year here according to what you want\n",
        "REMOTE_DIR = urljoin(BASE_URL, PRODUCT)\n",
        "\n",
        "# Your output directory\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# AUTH: load ~/.netrc OR getpass() - the former is preferred, latter is buggy\n",
        "def get_earthdata_session():\n",
        "    session = requests.Session()\n",
        "\n",
        "    netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "    if os.path.exists(netrc_path):\n",
        "        print(\"Using ~/.netrc for authentication.\")\n",
        "        # requests automatically uses .netrc\n",
        "        session.trust_env = True\n",
        "        return session\n",
        "\n",
        "    # Otherwise prompt to avoid exposing creds\n",
        "    print(\"No ~/.netrc file found â€“ prompting for Earthdata credentials.\")\n",
        "    username = input(\"Earthdata username: \")\n",
        "    password = getpass(\"Earthdata password: \")\n",
        "\n",
        "    session.auth = (username, password)\n",
        "    return session\n",
        "\n",
        "\n",
        "session = get_earthdata_session()\n",
        "\n",
        "\n",
        "# Discover remote nc4 files by scraping directory listing\n",
        "def list_remote_nc4():\n",
        "    r = session.get(REMOTE_DIR)\n",
        "    r.raise_for_status()\n",
        "    lines = r.text.splitlines()\n",
        "    files = []\n",
        "    for L in lines:\n",
        "        if \".nc4\" in L and 'href' in L:\n",
        "            name = L.split('href=\"')[1].split('\"')[0]\n",
        "            if name.endswith(\".nc4\"):\n",
        "                files.append(name)\n",
        "    print(f\"Found {len(files)} remote NC4 files.\")\n",
        "    return sorted(files)\n",
        "\n",
        "\n",
        "REMOTE_FILES = list_remote_nc4()\n",
        "\n",
        "\n",
        "# Chunked lazy reader (to minimize memory)\n",
        "def read_variable_chunked(h5file, var, chunk=50000):\n",
        "    \"\"\"Yield small slices of a dataset.\"\"\"\n",
        "    dset = h5file[var]\n",
        "    n = dset.shape[0]\n",
        "    for start in range(0, n, chunk):\n",
        "        end = min(start + chunk, n)\n",
        "        yield dset[start:end]\n",
        "\n",
        "\n",
        "# Stream download + lazy read + filter (SAM and quality flag)\n",
        "\n",
        "def read_filter_remote_file(filename):\n",
        "    url = urljoin(REMOTE_DIR, filename)\n",
        "    print(f\"Downloading + processing: {filename}\")\n",
        "\n",
        "    with session.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "\n",
        "        import io\n",
        "        bio = io.BytesIO(r.content)\n",
        "\n",
        "    with h5py.File(bio, 'r') as f:\n",
        "        ns = f['sounding_id'].shape[0]\n",
        "\n",
        "        # Create empty recarray - fields for CO2 data\n",
        "        dtype = [\n",
        "            ('sounding_id','int64'),\n",
        "            ('xco2','f8'),\n",
        "            ('operation_mode','S2'),\n",
        "            ('xco2_quality_flag','i1'),\n",
        "            ('target_name','S100'),\n",
        "            ('latitude','f8'),\n",
        "            ('longitude','f8')\n",
        "        ]\n",
        "        out = []\n",
        "\n",
        "        # Operation-mode mapping\n",
        "        mapping = {0:b'ND',1:b'GL',2:b'TG',3:b'XS',4:b'AM'}\n",
        "\n",
        "        # Iterate in chunks\n",
        "        for sid_chunk, x_chunk, op_chunk, q_chunk, tn_chunk, lat_chunk, lon_chunk in zip(\n",
        "            read_variable_chunked(f, 'sounding_id'),\n",
        "            read_variable_chunked(f, 'xco2'),\n",
        "            read_variable_chunked(f, 'Sounding/operation_mode'),\n",
        "            read_variable_chunked(f, 'xco2_quality_flag'),\n",
        "            read_variable_chunked(f, 'Sounding/target_name'),\n",
        "            read_variable_chunked(f, 'latitude'),\n",
        "            read_variable_chunked(f, 'longitude')\n",
        "        ):\n",
        "            # Convert operation_mode\n",
        "            op_decoded = np.array([mapping.get(int(v), b'UN') for v in op_chunk])\n",
        "\n",
        "            chunk_struct = np.zeros(len(sid_chunk), dtype=dtype)\n",
        "            chunk_struct['sounding_id'] = sid_chunk\n",
        "            chunk_struct['xco2'] = x_chunk\n",
        "            chunk_struct['operation_mode'] = op_decoded\n",
        "            chunk_struct['xco2_quality_flag'] = q_chunk\n",
        "            chunk_struct['target_name'] = tn_chunk\n",
        "            chunk_struct['latitude'] = lat_chunk\n",
        "            chunk_struct['longitude'] = lon_chunk\n",
        "\n",
        "            # Filter: AM + quality=0\n",
        "            mask = (chunk_struct['operation_mode'] == b'AM') & (chunk_struct['xco2_quality_flag'] == 0)\n",
        "            out.append(chunk_struct[mask])\n",
        "\n",
        "        if len(out) == 0:\n",
        "            return np.empty(0, dtype=dtype)\n",
        "\n",
        "        data = np.concatenate(out)\n",
        "\n",
        "    # Add datetime from sounding_id\n",
        "    dt_strings = np.array([str(s)[:14] for s in data['sounding_id']])\n",
        "    data = rfn.append_fields(\n",
        "        data,\n",
        "        'datetime',\n",
        "        pd.to_datetime(dt_strings, format='%Y%m%d%H%M%S'),\n",
        "        usemask=False\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "# Combine files into a single CSV\n",
        "def combine_remote_files(n_files=3, output_csv=\"combined_median.csv\"):\n",
        "    selected = REMOTE_FILES[:n_files]\n",
        "\n",
        "    all_data = []\n",
        "    for f in tqdm(selected):\n",
        "        data = read_filter_remote_file(f)\n",
        "        if len(data) > 0:\n",
        "            all_data.append(data)\n",
        "\n",
        "    if len(all_data) == 0:\n",
        "        print(\"No data found!\")\n",
        "        return\n",
        "\n",
        "    combined = np.concatenate(all_data)\n",
        "    df = pd.DataFrame(combined)\n",
        "\n",
        "    agg = df.groupby('target_name', as_index=False).agg({\n",
        "        'xco2':'median',\n",
        "        'latitude':'mean',\n",
        "        'longitude':'mean',\n",
        "        'datetime':'first',\n",
        "        'operation_mode':'first',\n",
        "        'xco2_quality_flag':'first'\n",
        "    })\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_DIR, output_csv)\n",
        "    agg.to_csv(out_path, index=False)\n",
        "    print(f\"Saved: {out_path}\")\n",
        "    return agg\n"
      ],
      "metadata": {
        "id": "wbcRrlAVpmCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After \"Found ____ remote NC4 files\" is printed, which means that the Earthdata credentials succeeded, run this\n",
        "# Amend n_files to the number of files you want to test/ process\n",
        "\n",
        "combine_remote_files(n_files=243) #change this number and match the number output above if you want all files processed"
      ],
      "metadata": {
        "id": "V70mw-WqqSLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. NC4 Files (local/ gdrive) to CSV**\n",
        "\n",
        "This is a tool to filter for high quality SAMs, and to aggregate a single CO2 value for each SAM location (taking the median CO2), from a folder on google drive of multiple nc4s. The lat/lon is the center point of all the soundings with the same target_name. This requires prior local download of nc4 files, see README of this github repo for instructions."
      ],
      "metadata": {
        "id": "SZVFcUK43QIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from ipywidgets import interact, IntSlider\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# PATHS - replace with your own google drive shortcut to D2D shared drive\n",
        "DATA_DIR = \"/content/drive/MyDrive/Shortcuts/DATA/2024-CO2-netcdfs\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Shortcuts/DATA/output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Discover NC4 files\n",
        "files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.nc4')])\n",
        "print(f\"Found {len(files)} NC4 files.\")\n",
        "\n",
        "\n",
        "# INTERACTIVE FILE SELECTION\n",
        "@interact(n_files=IntSlider(min=1, max=len(files), step=1, value=3, description='Files to process'))\n",
        "def select_files(n_files):\n",
        "    global SELECTED_FILES\n",
        "    SELECTED_FILES = files[:n_files]\n",
        "    print(f\"Selected {len(SELECTED_FILES)} files:\")\n",
        "    for f in SELECTED_FILES:\n",
        "        print(\" -\", f)\n",
        "\n",
        "\n",
        "# READ AND FILTER ONE FILE\n",
        "def read_filter_file(path):\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        ns = len(f['sounding_id'])\n",
        "        data = np.zeros(ns, dtype=[('sounding_id','int64'),\n",
        "                                   ('xco2','f8'),\n",
        "                                   ('operation_mode','S2'),\n",
        "                                   ('xco2_quality_flag','i1'),\n",
        "                                   ('target_name','S100'),\n",
        "                                   ('latitude','f8'),\n",
        "                                   ('longitude','f8')])\n",
        "        # Direct extraction\n",
        "        data['sounding_id'] = f['sounding_id'][...]\n",
        "        data['xco2'] = f['xco2'][...]\n",
        "        op = f['Sounding/operation_mode'][...]\n",
        "        mapping = {0:b'ND',1:b'GL',2:b'TG',3:b'XS',4:b'AM'}\n",
        "        data['operation_mode'] = np.array([mapping.get(int(v), b'UNK') for v in op])\n",
        "        data['xco2_quality_flag'] = f['xco2_quality_flag'][...]\n",
        "        data['target_name'] = f['Sounding/target_name'][...]\n",
        "        data['latitude'] = f['latitude'][...]\n",
        "        data['longitude'] = f['longitude'][...]\n",
        "\n",
        "    # Filter: AM + quality_flag 0\n",
        "    mask = (data['operation_mode'] == b'AM') & (data['xco2_quality_flag'] == 0)\n",
        "    data = data[mask]\n",
        "\n",
        "    # Add datetime (optional, per sounding)\n",
        "    dt_strings = np.array([str(s)[:14] for s in data['sounding_id']])\n",
        "    data = np.lib.recfunctions.append_fields(data, 'datetime',\n",
        "                                             pd.to_datetime(dt_strings, format='%Y%m%d%H%M%S'),\n",
        "                                             usemask=False)\n",
        "    return data\n",
        "\n",
        "# COMBINE FILES INTO ONE CSV\n",
        "def combine_files(output_csv=\"combined_median.csv\"):\n",
        "    import numpy.lib.recfunctions as rfn\n",
        "    all_data = []\n",
        "\n",
        "    for f in tqdm(SELECTED_FILES, desc=f\"Processing {len(SELECTED_FILES)} files\"):\n",
        "        path = os.path.join(DATA_DIR, f)\n",
        "        data = read_filter_file(path)\n",
        "        all_data.append(data)\n",
        "\n",
        "    if len(all_data) == 0:\n",
        "        print(\"No data to combine!\")\n",
        "        return\n",
        "\n",
        "    combined = np.concatenate(all_data)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(combined)\n",
        "\n",
        "    # Aggregate by target_name\n",
        "    agg_df = df.groupby('target_name', as_index=False).agg({\n",
        "        'xco2':'median',\n",
        "        'latitude':'mean',\n",
        "        'longitude':'mean',\n",
        "        'datetime':'first',\n",
        "        'operation_mode':'first',   # keep a representative value\n",
        "        'xco2_quality_flag':'first' # keep a representative value\n",
        "    })\n",
        "\n",
        "    # Save CSV\n",
        "    out_path = os.path.join(OUTPUT_DIR, output_csv)\n",
        "    agg_df.to_csv(out_path, index=False)\n",
        "    print(f\"Saved combined CSV: {out_path}\")\n"
      ],
      "metadata": {
        "id": "KJxFYZ_5A0jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After choosing number of files in the slider above, run this\n",
        "\n",
        "combine_files()"
      ],
      "metadata": {
        "id": "63F_N1tOBMKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **3. Data Exploration of NC4 File Variables**\n",
        "\n",
        "This is a tool to interactively select variables in a SINGLE nc4 file, allowing one to preview all nested variables in an nc4 file including hidden subfolders, and to plot them into a CSV for further processing. This is more for **data exploration** than data processing since individual csvs produced here still have large file sizes."
      ],
      "metadata": {
        "id": "n53fYInmzwai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "from ipywidgets import interact, IntSlider, SelectMultiple\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Base folder containing .nc4 files (replace with your own google drive shortcut to D2D shared drive)\n",
        "DATA_DIR = \"/content/drive/MyDrive/Shortcuts/DATA/2025-CO2-netcdfs\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Shortcuts/DATA/output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Discover years and files\n",
        "files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".nc4\")]\n",
        "years = sorted(list({f.split('_')[2][:2] for f in files}))  # extract '24' from '240716'\n",
        "print(f\"Found {len(files)} total NC4 files.\")\n"
      ],
      "metadata": {
        "id": "br2lxwKiC7Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample files per year\n",
        "files_by_year = {}\n",
        "for f in files:\n",
        "    year = \"20\" + f.split('_')[2][:2]\n",
        "    files_by_year.setdefault(year, []).append(f)\n",
        "\n",
        "available_years = sorted(files_by_year.keys())\n",
        "print(\"Available years:\", available_years)\n",
        "\n",
        "# Let user choose interactively\n",
        "@interact(\n",
        "    year=available_years,\n",
        "    n_files=IntSlider(min=1, max=10, step=1, value=3, description='Files to process'),\n",
        ")\n",
        "def choose_year(year, n_files):\n",
        "    chosen = files_by_year[year][:n_files]\n",
        "    print(f\"Will process {len(chosen)} files from {year}:\")\n",
        "    for f in chosen:\n",
        "        print(\" -\", f)\n",
        "    globals()['SELECTED_FILES'] = chosen\n"
      ],
      "metadata": {
        "id": "if7Ul_mAGUyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recursively collect dataset paths in an HDF5 file\n",
        "def collect_datasets(h5file):\n",
        "    dataset_paths = []\n",
        "\n",
        "    def visitor(name, obj):\n",
        "        if isinstance(obj, h5py.Dataset):\n",
        "            dataset_paths.append(name)\n",
        "    h5file.visititems(visitor)\n",
        "    return dataset_paths\n",
        "\n",
        "# First, pick a sample file to list variables\n",
        "sample_file = os.path.join(DATA_DIR, SELECTED_FILES[0]) # test 1 file\n",
        "# sample_file = os.path.join(DATA_DIR, files[0]) # all files\n",
        "with h5py.File(sample_file, 'r') as f:\n",
        "    all_datasets = collect_datasets(f)\n",
        "\n",
        "# Let user select variables interactively\n",
        "\n",
        "preselected_vars = [\n",
        "    'sounding_id',\n",
        "    'Sounding/operation_mode',\n",
        "    'xco2',\n",
        "    'xco2_quality_flag',\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "    'Sounding/target_name'\n",
        "]\n",
        "\n",
        "@interact(\n",
        "    variables=SelectMultiple(\n",
        "        options=all_datasets,\n",
        "        value=[v for v in preselected_vars if v in all_datasets],\n",
        "        description='Variables',\n",
        "        layout={'width': 'max-content'},\n",
        "        rows=15\n",
        "    )\n",
        ")\n",
        "def choose_variables(variables):\n",
        "    print(\"Selected variables for CSV extraction:\")\n",
        "    for v in variables:\n",
        "        print(\" -\", v)\n",
        "    globals()['SELECTED_VARS'] = variables\n"
      ],
      "metadata": {
        "id": "y5XS1DN-jUBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get time from sounding_id and filter functions for good quality SAMs\n",
        "\n",
        "def filter_and_extract_time(df):\n",
        "    \"\"\"\n",
        "    Filter the DataFrame for SAM/AM mode and good quality, and extract datetime from sounding_id.\n",
        "    \"\"\"\n",
        "    # Ensure sounding_id is string\n",
        "    df['sounding_id'] = df['sounding_id'].astype(str)\n",
        "\n",
        "    # Extract datetime from sounding_id (YYYYMMDDHHMM)\n",
        "    df['year'] = df['sounding_id'].str[:4].astype(int)\n",
        "    df['month'] = df['sounding_id'].str[4:6].astype(int)\n",
        "    df['day'] = df['sounding_id'].str[6:8].astype(int)\n",
        "    df['hour'] = df['sounding_id'].str[8:10].astype(int)\n",
        "    df['minute'] = df['sounding_id'].str[10:12].astype(int)\n",
        "    df['datetime'] = pd.to_datetime(df[['year','month','day','hour','minute']])\n",
        "\n",
        "    # Filter for SAM (AM) only\n",
        "    df = df[df['sounding_operation_mode'] == 'AM']\n",
        "\n",
        "    # Filter for good quality flag\n",
        "    df = df[df['xco2_quality_flag'] == 0]\n",
        "\n",
        "    # Drop intermediate columns\n",
        "    df = df.drop(columns=['year','month','day','hour','minute'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Rm4K2Kh1ss4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV file extraction\n",
        "\n",
        "def process_nc4_file(local_path, variables): # all files\n",
        "    filename = os.path.basename(local_path)\n",
        "    csv_name = os.path.join(OUTPUT_DIR, filename.replace(\".nc4\", \".csv\"))\n",
        "\n",
        "    try:\n",
        "        with h5py.File(local_path, 'r') as f:\n",
        "            out_dict = {}\n",
        "            for var in variables:\n",
        "                # Direct access\n",
        "                if var in f:\n",
        "                    out_dict[var.replace('/', '_')] = f[var][...]\n",
        "                else:\n",
        "                    # Search for nested dataset ending with last component of var\n",
        "                    var_name = var.split('/')[-1]\n",
        "                    found = False\n",
        "                    def find_dataset(name, obj):\n",
        "                        nonlocal found\n",
        "                        if isinstance(obj, h5py.Dataset) and name.endswith(var_name):\n",
        "                            out_dict[name.replace('/', '_')] = obj[...]\n",
        "                            found = True\n",
        "                    f.visititems(find_dataset)\n",
        "                    if not found:\n",
        "                        print(f\"Warning: variable {var} not found in {filename}\")\n",
        "\n",
        "            # Map Sounding/operation_mode integers to strings if present\n",
        "            key_opmode = [k for k in out_dict.keys() if k.endswith('operation_mode')]\n",
        "            if key_opmode:\n",
        "                k = key_opmode[0]\n",
        "                mapping = {0: 'ND', 1: 'GL', 2: 'TG', 3: 'XS', 4: 'AM'}\n",
        "                out_dict[k] = [mapping.get(int(v), 'UNK') for v in out_dict[k]]\n",
        "\n",
        "            df = pd.DataFrame(out_dict)\n",
        "\n",
        "            # Filter for SAM mode (AM) and good quality, and extract datetime\n",
        "            if 'sounding_operation_mode' in df.columns and 'xco2_quality_flag' in df.columns:\n",
        "                df = df[(df['sounding_operation_mode'] == 'AM') & (df['xco2_quality_flag'] == 0)]\n",
        "            if 'sounding_id' in df.columns:\n",
        "                df['datetime'] = df['sounding_id'].astype(str).apply(\n",
        "                    lambda x: f\"20{x[:2]}-{x[2:4]}-{x[4:6]} {x[8:10]}:{x[10:12]}:00\"\n",
        "                )\n",
        "\n",
        "            df.to_csv(csv_name, index=False)\n",
        "            print(f\"Saved CSV: {csv_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed {local_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "nQeUnj3VGiV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all():\n",
        "    for f in tqdm(SELECTED_FILES, desc=f\"Processing {len(SELECTED_FILES)} files\"):\n",
        "        local_path = os.path.join(DATA_DIR, f)\n",
        "        process_nc4_file(local_path, SELECTED_VARS)\n",
        "\n",
        "run_all()\n"
      ],
      "metadata": {
        "id": "1yXO9MT6Hky_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}