{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heytian/d2d-oco3-tools/blob/main/nc4-extract-NDGL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bnrZ15jjc9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-nlAljK954M"
      },
      "source": [
        "# **NC4 to CSV - CO2**\n",
        "\n",
        "Work in progress as of Feb 25, 2026\n",
        "\n",
        "This script batch processes netcdf (.nc4) files of OCO-3 CO2 Lite data (from https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/OCO3_L2_Lite_FP.11r/) to create a csv for SAM, referencing the Nadir & Glint CO2 working code from Feb 19, 2026 (https://github.com/heytian/d2d-oco3-tools/blob/main/nc4-extract-NDGL.ipynb).\n",
        "\n",
        "If \"no data found\", try clearing browser history for the day, and also double check that Earth Data credentials work when directly downloading from the gesdisc source.  \n",
        "\n",
        "**IMPORTANT: Clear all outputs and end runtime session before saving to Colab or Github to avoid exposing your Earthdata credentials!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xY7cYj5PRg_"
      },
      "outputs": [],
      "source": [
        "# Run this to write .netrc to Colab for your Earthdata credentials. RESTART SESSION AND CLEAR OUTPUTS AFTER USE!\n",
        "\n",
        "import getpass, os\n",
        "\n",
        "u = getpass.getpass(\"Earthdata Username: \")\n",
        "p = getpass.getpass(\"Earthdata Password: \")\n",
        "\n",
        "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "with open(netrc_path, \"w\") as f:\n",
        "    f.write(f\"machine urs.earthdata.nasa.gov\\n\"\n",
        "            f\"  login {u}\\n\"\n",
        "            f\"  password {p}\\n\")\n",
        "\n",
        "os.chmod(netrc_path, 0o600)\n",
        "\n",
        "cookie_path = os.path.expanduser(\"~/.urs_cookies\")\n",
        "open(cookie_path, \"a\").close()\n",
        "os.chmod(cookie_path, 0o600)\n",
        "\n",
        "print(\"Credentials loaded securely.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5_DxNV2OXXL"
      },
      "outputs": [],
      "source": [
        "!pip install pycountry\n",
        "!pip install timezonefinder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niXTDITY7ZOl"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8x98CgV-Myl"
      },
      "source": [
        "20260219 1715h ET integrated spatial join with Rob's centroids; working!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CinZSP2-Lt0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import time\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "from tqdm import tqdm\n",
        "from getpass import getpass\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pycountry\n",
        "from functools import partial\n",
        "import numpy.lib.recfunctions as rfn\n",
        "from timezonefinder import TimezoneFinder\n",
        "import pytz\n",
        "\n",
        "\n",
        "BASE_URL = \"https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/\"\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "ref_data = pd.read_csv(\"/content/drive/MyDrive/Shortcuts/csv_xlsx/20260129_from_Rob/clasp_report.csv\")  # Rob's centroid csv; amend location to your local file path\n",
        "ref_geodata = gpd.GeoDataFrame(\n",
        "    ref_data,\n",
        "    geometry=ref_data[\"Site Shape WKT\"].apply(wkt.loads),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_earthdata_session():\n",
        "    session = requests.Session()\n",
        "    netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "    if os.path.exists(netrc_path):\n",
        "        session.trust_env = True\n",
        "        return session\n",
        "    username = input(\"Earthdata username: \")\n",
        "    password = getpass(\"Earthdata password: \")\n",
        "    session.auth = (username, password)\n",
        "    return session\n",
        "\n",
        "session = get_earthdata_session()\n",
        "\n",
        "def list_remote_nc4(product_dir):\n",
        "    try:\n",
        "        r = session.get(product_dir)\n",
        "        r.raise_for_status()\n",
        "        lines = r.text.splitlines()\n",
        "        files = [\n",
        "            L.split('href=\"')[1].split('\"')[0]\n",
        "            for L in lines\n",
        "            if \".nc4\" in L and not L.strip().endswith('.xml') and 'href' in L\n",
        "        ]\n",
        "        return sorted(files)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def safe_open_h5(bio):\n",
        "    bio.seek(0)\n",
        "    first_bytes = bio.read(15)\n",
        "    bio.seek(0)\n",
        "    if first_bytes.startswith(b'<!DOCTYPE') or first_bytes.startswith(b'<html'):\n",
        "        raise OSError(\"Downloaded file is HTML, not HDF5\")\n",
        "    sig = bio.read(8)\n",
        "    bio.seek(0)\n",
        "    if sig != b'\\x89HDF\\r\\n\\x1a\\n':\n",
        "        raise OSError(\"Not a valid HDF5 file\")\n",
        "    return h5py.File(bio, 'r')\n",
        "\n",
        "def parse_city_country_pycountry(target_names):\n",
        "    s = target_names.str.replace('fossil_', '', regex=False)\n",
        "    parts = s.str.split('_')\n",
        "    country_names = {c.name.lower() for c in pycountry.countries}\n",
        "    country_names.update({getattr(c, \"common_name\", \"\").lower() for c in pycountry.countries if hasattr(c, \"common_name\")})\n",
        "    def get_city_country(lst):\n",
        "        for n in range(3, 0, -1):\n",
        "            if len(lst) < n:\n",
        "                continue\n",
        "            candidate = ' '.join(lst[-n:]).lower()\n",
        "            if candidate in country_names:\n",
        "                city = ' '.join(lst[:-n])\n",
        "                country = ' '.join(lst[-n:])\n",
        "                return city, country\n",
        "        return ' '.join(lst[:-1]), lst[-1]\n",
        "    city_country = parts.apply(get_city_country)\n",
        "    city = city_country.apply(lambda x: x[0])\n",
        "    country = city_country.apply(lambda x: x[1])\n",
        "    return city, country\n",
        "\n",
        "def read_variable_chunked(h5file, var, chunk=50000):\n",
        "    dset = h5file[var]\n",
        "    n = dset.shape[0]\n",
        "    for start in range(0, n, chunk):\n",
        "        end = min(start + chunk, n)\n",
        "        yield dset[start:end]\n",
        "\n",
        "tf = TimezoneFinder()\n",
        "\n",
        "def read_filter_remote_file(filename, product_dir, retries=3, fossil_only=True):\n",
        "    mapping = {0:b'ND', 1:b'GL', 2:b'TG', 3:b'XS', 4:b'AM'}\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            url = urljoin(product_dir, filename)\n",
        "            with session.get(url, stream=True, timeout=120) as r:\n",
        "                r.raise_for_status()\n",
        "                bio = io.BytesIO(r.content)\n",
        "\n",
        "            with safe_open_h5(bio) as f:\n",
        "                dfs = []\n",
        "\n",
        "                for sid_chunk, x_chunk, op_chunk, q_chunk, tn_chunk, lat_chunk, lon_chunk in zip(\n",
        "                    read_variable_chunked(f, 'sounding_id'),\n",
        "                    read_variable_chunked(f, 'xco2'),\n",
        "                    read_variable_chunked(f, 'Sounding/operation_mode'),\n",
        "                    read_variable_chunked(f, 'xco2_quality_flag'),\n",
        "                    read_variable_chunked(f, 'Sounding/target_name'),\n",
        "                    read_variable_chunked(f, 'latitude'),\n",
        "                    read_variable_chunked(f, 'longitude')\n",
        "                ):\n",
        "                    op_decoded = np.array([mapping.get(int(v), b'UN') for v in op_chunk])\n",
        "\n",
        "                    df_chunk = pd.DataFrame({\n",
        "                        \"sounding_id\": sid_chunk,\n",
        "                        \"xco2\": x_chunk,\n",
        "                        \"operation_mode\": op_decoded,\n",
        "                        \"xco2_quality_flag\": q_chunk,\n",
        "                        \"target_name\": [t.decode(\"utf-8\").strip() for t in tn_chunk],\n",
        "                        \"latitude\": lat_chunk,\n",
        "                        \"longitude\": lon_chunk\n",
        "                    })\n",
        "\n",
        "                    # ND/GL + good quality\n",
        "                    df_chunk = df_chunk[\n",
        "                        df_chunk[\"operation_mode\"].isin([b\"ND\", b\"GL\"]) &\n",
        "                        (df_chunk[\"xco2_quality_flag\"] == 0)\n",
        "                    ]\n",
        "\n",
        "                    if df_chunk.empty:\n",
        "                        continue\n",
        "\n",
        "                    # Spatial filtering\n",
        "                    points_gdf = gpd.GeoDataFrame(\n",
        "                        df_chunk,\n",
        "                        geometry=gpd.points_from_xy(df_chunk.longitude, df_chunk.latitude),\n",
        "                        crs=\"EPSG:4326\"\n",
        "                    )\n",
        "\n",
        "                    joined = gpd.sjoin(\n",
        "                        points_gdf,\n",
        "                        ref_geodata[[\"Target Name\", \"geometry\"]],\n",
        "                        how=\"inner\",\n",
        "                        predicate=\"within\"\n",
        "                    )\n",
        "\n",
        "                    if not joined.empty:\n",
        "                        dfs.append(joined.drop(columns=\"geometry\"))\n",
        "\n",
        "                if not dfs:\n",
        "                    return None\n",
        "\n",
        "                df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "                # Fossil filter\n",
        "                if fossil_only:\n",
        "                    df_all = df_all[df_all[\"Target Name\"].str.startswith(\"fossil\")]\n",
        "\n",
        "                if df_all.empty:\n",
        "                    return None\n",
        "\n",
        "                # Add datetime\n",
        "                df_all[\"datetime\"] = pd.to_datetime(\n",
        "                    df_all[\"sounding_id\"].astype(str).str[:14],\n",
        "                    format=\"%Y%m%d%H%M%S\"\n",
        "                )\n",
        "\n",
        "                # Add local time\n",
        "                local_times = []\n",
        "                tz_abbrs = []\n",
        "\n",
        "                for lon, lat, utc_dt in zip(df_all.longitude, df_all.latitude, df_all.datetime):\n",
        "                    tz_name = tf.timezone_at(lng=lon, lat=lat)\n",
        "                    utc_dt = utc_dt.tz_localize(\"UTC\")\n",
        "\n",
        "                    if tz_name:\n",
        "                        local_dt = utc_dt.tz_convert(tz_name)\n",
        "                        local_times.append(local_dt.strftime('%m/%d/%y %H:%M'))\n",
        "                        tz_abbrs.append(local_dt.strftime('%Z'))\n",
        "                    else:\n",
        "                        local_times.append(utc_dt.strftime('%m/%d/%y %H:%M'))\n",
        "                        tz_abbrs.append(\"UTC\")\n",
        "\n",
        "                df_all[\"local_time\"] = local_times\n",
        "                df_all[\"timezone\"] = tz_abbrs\n",
        "\n",
        "                return df_all\n",
        "\n",
        "        except Exception:\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "def combine_remote_files_years(years, product_template=\"OCO3_L2_Lite_FP.11r/{year}/\", n_files=None, output_csv=\"CO2_NDGL.csv\", max_workers=4):\n",
        "    all_data = []\n",
        "    for year in years:\n",
        "        product_dir = urljoin(BASE_URL, product_template.format(year=year))\n",
        "        remote_files = list_remote_nc4(product_dir)\n",
        "        if not remote_files:\n",
        "            continue\n",
        "        selected = remote_files if n_files is None else remote_files[:n_files]\n",
        "        func = partial(read_filter_remote_file, product_dir=product_dir)\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            results = list(tqdm(executor.map(func, selected), total=len(selected)))\n",
        "            for r in results:\n",
        "                if r is not None and not r.empty:\n",
        "                    all_data.append(r)\n",
        "    if not all_data:\n",
        "        print(\"No data found!\")\n",
        "        return None\n",
        "\n",
        "    df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    # df = pd.DataFrame(combined)\n",
        "    df['target_name'] = df['target_name'].apply(lambda x: x.decode('utf-8') if isinstance(x, (bytes, bytearray)) else x)\n",
        "    city, country = parse_city_country_pycountry(df['target_name'])\n",
        "    df['city'] = city\n",
        "    df['country'] = country\n",
        "    out_path = os.path.join(OUTPUT_DIR, output_csv)\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(out_path)\n",
        "    return df\n",
        "\n",
        "\n",
        "years = [2019,2020,2021,2022,2023,2024,2025]\n",
        "combine_remote_files_years(years, n_files=None, max_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokarcxS99de"
      },
      "source": [
        "# **NC4 to CSV - SIF**\n",
        "\n",
        "(As of Feb 25 2026, yet to be written)\n",
        "\n",
        "This script batch processes netcdf (.nc4) files of OCO-3 SIF Lite data (from https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/OCO3_L2_Lite_SIF.11r/) to create a csv containing SAM data.\n",
        "\n",
        "\n",
        "**IMPORTANT: Clear all outputs and end runtime session before saving to Colab or Github to avoid exposing your Earthdata credentials!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
