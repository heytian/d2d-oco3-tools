{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heytian/d2d-oco3-tools/blob/main/nc4-extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bnrZ15jjc9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-nlAljK954M"
      },
      "source": [
        "# **NC4 to CSV - CO2**\n",
        "\n",
        "This script batch processes netcdf (.nc4) files of OCO-3 CO2 Lite data (from https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/OCO3_L2_Lite_FP.11r/) to create a csv containing high quality SAMs tagged as \"fossil\".\n",
        "\n",
        "**IMPORTANT: Clear all outputs and end runtime session before saving to Colab or Github to avoid exposing your Earthdata credentials!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xY7cYj5PRg_"
      },
      "outputs": [],
      "source": [
        "# Run this to write .netrc to Colab for your Earthdata credentials. RESTART SESSION AND CLEAR OUTPUTS AFTER USE!\n",
        "\n",
        "import getpass, os\n",
        "\n",
        "u = getpass.getpass(\"Earthdata Username: \")\n",
        "p = getpass.getpass(\"Earthdata Password: \")\n",
        "\n",
        "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "with open(netrc_path, \"w\") as f:\n",
        "    f.write(f\"machine urs.earthdata.nasa.gov\\n\"\n",
        "            f\"  login {u}\\n\"\n",
        "            f\"  password {p}\\n\")\n",
        "\n",
        "os.chmod(netrc_path, 0o600)\n",
        "\n",
        "cookie_path = os.path.expanduser(\"~/.urs_cookies\")\n",
        "open(cookie_path, \"a\").close()\n",
        "os.chmod(cookie_path, 0o600)\n",
        "\n",
        "print(\"Credentials loaded securely.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5_DxNV2OXXL"
      },
      "outputs": [],
      "source": [
        "!pip install pycountry"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20260219 fix below:"
      ],
      "metadata": {
        "id": "J6xu7orX_FKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import time\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from getpass import getpass\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pycountry\n",
        "from functools import partial\n",
        "import numpy.lib.recfunctions as rfn\n",
        "\n",
        "BASE_URL = \"https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/\"\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def get_earthdata_session():\n",
        "    session = requests.Session()\n",
        "    netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "    if os.path.exists(netrc_path):\n",
        "        session.trust_env = True\n",
        "        return session\n",
        "    username = input(\"Earthdata username: \")\n",
        "    password = getpass(\"Earthdata password: \")\n",
        "    session.auth = (username, password)\n",
        "    return session\n",
        "\n",
        "session = get_earthdata_session()\n",
        "\n",
        "def list_remote_nc4(product_dir):\n",
        "    try:\n",
        "        r = session.get(product_dir)\n",
        "        r.raise_for_status()\n",
        "        lines = r.text.splitlines()\n",
        "        files = [\n",
        "            L.split('href=\"')[1].split('\"')[0]\n",
        "            for L in lines\n",
        "            if \".nc4\" in L and not L.strip().endswith('.xml') and 'href' in L\n",
        "        ]\n",
        "        return sorted(files)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def safe_open_h5(bio):\n",
        "    bio.seek(0)\n",
        "    first_bytes = bio.read(15)\n",
        "    bio.seek(0)\n",
        "    if first_bytes.startswith(b'<!DOCTYPE') or first_bytes.startswith(b'<html'):\n",
        "        raise OSError(\"Downloaded file is HTML, not HDF5\")\n",
        "    sig = bio.read(8)\n",
        "    bio.seek(0)\n",
        "    if sig != b'\\x89HDF\\r\\n\\x1a\\n':\n",
        "        raise OSError(\"Not a valid HDF5 file\")\n",
        "    return h5py.File(bio, 'r')\n",
        "\n",
        "def parse_city_country_pycountry(target_names):\n",
        "    s = target_names.str.replace('fossil_', '', regex=False)\n",
        "    parts = s.str.split('_')\n",
        "    country_names = {c.name.lower() for c in pycountry.countries}\n",
        "    country_names.update({getattr(c, \"common_name\", \"\").lower() for c in pycountry.countries if hasattr(c, \"common_name\")})\n",
        "    def get_city_country(lst):\n",
        "        for n in range(3, 0, -1):\n",
        "            if len(lst) < n:\n",
        "                continue\n",
        "            candidate = ' '.join(lst[-n:]).lower()\n",
        "            if candidate in country_names:\n",
        "                city = ' '.join(lst[:-n])\n",
        "                country = ' '.join(lst[-n:])\n",
        "                return city, country\n",
        "        return ' '.join(lst[:-1]), lst[-1]\n",
        "    city_country = parts.apply(get_city_country)\n",
        "    city = city_country.apply(lambda x: x[0])\n",
        "    country = city_country.apply(lambda x: x[1])\n",
        "    return city, country\n",
        "\n",
        "def read_variable_chunked(h5file, var, chunk=50000):\n",
        "    dset = h5file[var]\n",
        "    n = dset.shape[0]\n",
        "    for start in range(0, n, chunk):\n",
        "        end = min(start + chunk, n)\n",
        "        yield dset[start:end]\n",
        "\n",
        "def read_filter_remote_file(filename, product_dir, retries=3):\n",
        "    dtype = [\n",
        "        ('sounding_id','int64'),\n",
        "        ('xco2','f8'),\n",
        "        ('operation_mode','S2'),\n",
        "        ('xco2_quality_flag','i1'),\n",
        "        ('target_name','S100'),\n",
        "        ('latitude','f8'),\n",
        "        ('longitude','f8')\n",
        "    ]\n",
        "    mapping = {0:b'ND',1:b'GL',2:b'TG',3:b'XS',4:b'AM'}\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            url = urljoin(product_dir, filename)\n",
        "            with session.get(url, stream=True, timeout=120) as r:\n",
        "                r.raise_for_status()\n",
        "                bio = io.BytesIO(r.content)\n",
        "            with safe_open_h5(bio) as f:\n",
        "                dtype = [\n",
        "                    ('sounding_id','int64'),\n",
        "                    ('xco2','f8'),\n",
        "                    ('operation_mode','S2'),\n",
        "                    ('xco2_quality_flag','i1'),\n",
        "                    ('target_name','S100'),\n",
        "                    ('latitude','f8'),\n",
        "                    ('longitude','f8')\n",
        "                ]\n",
        "                out = []\n",
        "                for sid_chunk, x_chunk, op_chunk, q_chunk, tn_chunk, lat_chunk, lon_chunk in zip(\n",
        "                    read_variable_chunked(f, 'sounding_id'),\n",
        "                    read_variable_chunked(f, 'xco2'),\n",
        "                    read_variable_chunked(f, 'Sounding/operation_mode'),\n",
        "                    read_variable_chunked(f, 'xco2_quality_flag'),\n",
        "                    read_variable_chunked(f, 'Sounding/target_name'),\n",
        "                    read_variable_chunked(f, 'latitude'),\n",
        "                    read_variable_chunked(f, 'longitude')\n",
        "                ):\n",
        "                    op_decoded = np.array([mapping.get(int(v), b'UN') for v in op_chunk])\n",
        "                    chunk_struct = np.zeros(len(sid_chunk), dtype=dtype)\n",
        "                    chunk_struct['sounding_id'] = sid_chunk\n",
        "                    chunk_struct['xco2'] = x_chunk\n",
        "                    chunk_struct['operation_mode'] = op_decoded\n",
        "                    chunk_struct['xco2_quality_flag'] = q_chunk\n",
        "                    chunk_struct['target_name'] = tn_chunk\n",
        "                    chunk_struct['latitude'] = lat_chunk\n",
        "                    chunk_struct['longitude'] = lon_chunk\n",
        "                    mask = (chunk_struct['operation_mode'] == b'AM') & (chunk_struct['xco2_quality_flag'] == 0)\n",
        "                    out.append(chunk_struct[mask])\n",
        "                if len(out) == 0:\n",
        "                    return np.empty(0, dtype=dtype)\n",
        "                data = np.concatenate(out)\n",
        "            dt_strings = np.array([str(s)[:14] for s in data['sounding_id']])\n",
        "            data = rfn.append_fields(data, 'datetime', pd.to_datetime(dt_strings, format='%Y%m%d%H%M%S'), usemask=False)\n",
        "            return data\n",
        "        except (requests.HTTPError, OSError, KeyError):\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "    return np.empty(0, dtype=dtype)\n",
        "    return\n",
        "\n",
        "def combine_remote_files_years(years, product_template=\"OCO3_L2_Lite_FP.11r/{year}/\", n_files=None, output_csv=\"CO2_combined.csv\", max_workers=4):\n",
        "    all_data = []\n",
        "    for year in years:\n",
        "        product_dir = urljoin(BASE_URL, product_template.format(year=year))\n",
        "        remote_files = list_remote_nc4(product_dir)\n",
        "        if not remote_files:\n",
        "            continue\n",
        "        selected = remote_files if n_files is None else remote_files[:n_files]\n",
        "        func = partial(read_filter_remote_file, product_dir=product_dir)\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            results = list(tqdm(executor.map(func, selected), total=len(selected)))\n",
        "            for r in results:\n",
        "                if len(r) > 0:\n",
        "                    all_data.append(r)\n",
        "    if not all_data:\n",
        "        print(\"No data found!\")\n",
        "        return None\n",
        "    combined = np.concatenate(all_data)\n",
        "    df = pd.DataFrame(combined)\n",
        "    df['target_name'] = df['target_name'].apply(lambda x: x.decode('utf-8') if isinstance(x, (bytes, bytearray)) else x)\n",
        "    df = df[df['target_name'].str.startswith('fossil_')]\n",
        "    city, country = parse_city_country_pycountry(df['target_name'])\n",
        "    df['city'] = city\n",
        "    df['country'] = country\n",
        "    agg = df.groupby(['target_name', 'datetime'], as_index=False).agg({\n",
        "        'xco2':'median',\n",
        "        'latitude':'mean',\n",
        "        'longitude':'mean',\n",
        "        'operation_mode':'first',\n",
        "        'xco2_quality_flag':'first',\n",
        "        'city':'first',\n",
        "        'country':'first'\n",
        "    })\n",
        "    out_path = os.path.join(OUTPUT_DIR, output_csv)\n",
        "    agg.to_csv(out_path, index=False)\n",
        "    print(out_path)\n",
        "    return agg\n",
        "\n",
        "# years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
        "# combine_remote_files_years(years, n_files=None, max_workers=2)\n",
        "\n",
        "\n",
        "years = [2025]\n",
        "combine_remote_files_years(years, n_files=10, max_workers=2)\n"
      ],
      "metadata": {
        "id": "CQ_01Uv1_EIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokarcxS99de"
      },
      "source": [
        "# **NC4 to CSV - SIF**\n",
        "\n",
        "This script batch processes netcdf (.nc4) files of OCO-3 SIF Lite data (from https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/OCO3_L2_Lite_SIF.11r/) to create a csv containing high quality SAMs.\n",
        "\n",
        "\n",
        "**IMPORTANT: Clear all outputs and end runtime session before saving to Colab or Github to avoid exposing your Earthdata credentials!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP3dJTHp-D57"
      },
      "outputs": [],
      "source": [
        "# Run this to write .netrc to Colab for your Earthdata credentials. RESTART SESSION AND CLEAR OUTPUTS AFTER USE!\n",
        "\n",
        "import getpass, os\n",
        "\n",
        "u = getpass.getpass(\"Earthdata Username: \")\n",
        "p = getpass.getpass(\"Earthdata Password: \")\n",
        "\n",
        "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "with open(netrc_path, \"w\") as f:\n",
        "    f.write(f\"machine urs.earthdata.nasa.gov\\n\"\n",
        "            f\"  login {u}\\n\"\n",
        "            f\"  password {p}\\n\")\n",
        "\n",
        "os.chmod(netrc_path, 0o600)\n",
        "\n",
        "cookie_path = os.path.expanduser(\"~/.urs_cookies\")\n",
        "open(cookie_path, \"a\").close()\n",
        "os.chmod(cookie_path, 0o600)\n",
        "\n",
        "print(\"Credentials loaded securely.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5vaXNBnKt6o"
      },
      "outputs": [],
      "source": [
        "pip install pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWCioeV4TFZA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import pycountry\n",
        "from tqdm import tqdm\n",
        "from getpass import getpass\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "\n",
        "BASE_URL = \"https://oco2.gesdisc.eosdis.nasa.gov/data/OCO3_DATA/\"\n",
        "PRODUCT_TEMPLATE = \"OCO3_L2_Lite_SIF.11r/{year}/\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Shortcuts/DATA/output\" # replace with your own path\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def get_earthdata_session():\n",
        "    session = requests.Session()\n",
        "    netrc_path = os.path.expanduser(\"~/.netrc\")\n",
        "    if os.path.exists(netrc_path):\n",
        "        session.trust_env = True\n",
        "        return session\n",
        "    username = input(\"Earthdata username: \")\n",
        "    password = getpass(\"Earthdata password: \")\n",
        "    session.auth = (username, password)\n",
        "    return session\n",
        "\n",
        "session = get_earthdata_session()\n",
        "\n",
        "def list_remote_nc4(product_dir):\n",
        "    r = session.get(product_dir, timeout=60)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    files = []\n",
        "    for line in r.text.splitlines():\n",
        "        if 'href=\"' not in line:\n",
        "            continue\n",
        "\n",
        "        href = line.split('href=\"')[1].split('\"')[0]\n",
        "\n",
        "        if href.endswith(\".nc4\") and not href.endswith(\".nc4.xml\"):\n",
        "            files.append(href)\n",
        "\n",
        "    print(f\"Found {len(files)} remote NC4 files.\")\n",
        "    return sorted(files)\n",
        "\n",
        "def safe_open_h5(bio):\n",
        "    bio.seek(0)\n",
        "    sig = bio.read(8)\n",
        "    bio.seek(0)\n",
        "    if sig != b'\\x89HDF\\r\\n\\x1a\\n':\n",
        "        raise OSError(\"Not a valid HDF5 file\")\n",
        "    return h5py.File(bio, 'r')\n",
        "\n",
        "def parse_city_country(df):\n",
        "    seq = df['sequence_name'].astype(str)\n",
        "    parts = seq.str.split('_')\n",
        "    df['city'] = parts.apply(lambda x: ' '.join(x[1:-1]) if len(x) > 2 else '')\n",
        "    def correct_country(c):\n",
        "        try:\n",
        "            return pycountry.countries.lookup(c).name\n",
        "        except LookupError:\n",
        "            return c\n",
        "    df['country'] = parts.apply(lambda x: x[-1] if len(x) > 1 else '')\n",
        "    return df\n",
        "\n",
        "def read_filter_remote_file(filename, product_dir, retries=3):\n",
        "    mapping = {0: b'ND', 1: b'GL', 2: b'TG', 3: b'AM', 4: b'XS'}\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            url = urljoin(product_dir, filename)\n",
        "            with session.get(url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                import io\n",
        "                bio = io.BytesIO(r.content)\n",
        "\n",
        "            with safe_open_h5(bio) as f:\n",
        "                sif = f['Daily_SIF_757nm'][:]\n",
        "                op_mode_arr = np.atleast_1d(f['Metadata/MeasurementMode'][:])\n",
        "                qflag = f['SimplyGoodOrBadQualityFlag'][:]\n",
        "                lat = f['Geolocation/latitude'][:]\n",
        "                lon = f['Geolocation/longitude'][:]\n",
        "                tai93 = f['Geolocation/time_tai93'][:]\n",
        "                seq_idx = f['Sequences/SequencesIndex'][:]\n",
        "                seq_names_all = f['Sequences/SequencesName'][:]\n",
        "\n",
        "                op_decoded = np.array([mapping.get(int(v), b'UN') for v in op_mode_arr])\n",
        "\n",
        "                # Filter for good quality SAMs\n",
        "                mask = (op_decoded == b'AM') & (qflag == 0)\n",
        "                if not np.any(mask):\n",
        "                    return None\n",
        "\n",
        "                sif = sif[mask]\n",
        "                op_decoded = op_decoded[mask]\n",
        "                lat = lat[mask]\n",
        "                lon = lon[mask]\n",
        "                tai93 = tai93[mask]\n",
        "                seq_idx = seq_idx[mask]\n",
        "\n",
        "                sequence_names = [\n",
        "                    seq_names_all[i].decode('utf-8') if 0 <= i < len(seq_names_all) else 'UNKNOWN'\n",
        "                    for i in seq_idx\n",
        "                ]\n",
        "\n",
        "                epoch = pd.Timestamp(\"1993-01-01T00:00:00Z\")\n",
        "                datetime = epoch + pd.to_timedelta(tai93.astype(float), unit=\"s\")\n",
        "\n",
        "                df = pd.DataFrame({\n",
        "                    'Daily_SIF_757nm': sif,\n",
        "                    'latitude': lat,\n",
        "                    'longitude': lon,\n",
        "                    'datetime': pd.to_datetime(datetime).tz_localize(None),\n",
        "                    'measurement_mode': [v.decode('utf-8') for v in op_decoded],\n",
        "                    'sequence_name': sequence_names\n",
        "                })\n",
        "\n",
        "                df = parse_city_country(df)\n",
        "\n",
        "                print(f\"{filename}: kept {len(df)} AM/SAM SIF soundings\")\n",
        "                return df\n",
        "\n",
        "        except (requests.HTTPError, OSError, KeyError) as e:\n",
        "            print(f\"Warning: {filename} attempt {attempt+1} failed: {e}\")\n",
        "            import time; time.sleep(2**attempt)\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "def combine_remote_files_years(years, n_files=None, max_workers=2, output_csv=\"combined_SIF_2019-2025.csv\"):\n",
        "    all_dfs = []\n",
        "    for year in years:\n",
        "        print(f\"\\nProcessing year: {year}\")\n",
        "        product_dir = urljoin(BASE_URL, PRODUCT_TEMPLATE.format(year=year))\n",
        "        remote_files = list_remote_nc4(product_dir)\n",
        "        selected_files = remote_files if n_files is None else remote_files[:n_files]\n",
        "\n",
        "        dfs = []\n",
        "        for f in tqdm(selected_files):\n",
        "            df = read_filter_remote_file(f, product_dir)\n",
        "            if df is not None and len(df) > 0:\n",
        "                dfs.append(df)\n",
        "\n",
        "        if dfs:\n",
        "            all_dfs.append(pd.concat(dfs, ignore_index=True))\n",
        "\n",
        "    if not all_dfs:\n",
        "        print(\"No data found!\")\n",
        "        return None\n",
        "\n",
        "    df_all = pd.concat(all_dfs, ignore_index=True)\n",
        "    out_path = os.path.join(OUTPUT_DIR, output_csv)\n",
        "    df_all.to_csv(out_path, index=False)\n",
        "    print(f\"\\nSaved combined SIF all-values CSV: {out_path}\")\n",
        "    return df_all\n",
        "\n",
        "years = [2019,2020,2022,2023,2024,2025]\n",
        "combine_remote_files_years(years, n_files=None, max_workers=2)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}